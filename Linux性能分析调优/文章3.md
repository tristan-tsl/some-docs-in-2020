https://lework.github.io/2020/01/11/io/

# Linux性能调优之 IO

2020-01-11

lework

 [Linux性能优化](https://lework.github.io/category/#Linux性能优化)

 [Linux性能优化](https://lework.github.io/tag/#Linux性能优化)

本文 10435 字，阅读全文约需 30 分钟

------

## I/O相关术语

### 索引节点

简称为 inode，用来记录文件的元数据，比如 inode 编号、文件大小、访问权限、修改日期、数据的位置等。索引节点和文件一一对应，它跟文件内容一样，都会被持久化存储到磁盘中。所以记住，索引节点同样占用磁盘空间。

### 目录项

简称为 dentry，用来记录文件的名字、索引节点指针以及与其他目录项的关联关系。多个关联的目录项，就构成了文件系统的目录结构。不过，不同于索引节点，目录项是由内核维护的一个内存数据结构，所以通常也被叫做目录项缓存。

> 索引节点是每个文件的唯一标志，而目录项维护的正是文件系统的树状结构。目录项和索引节点的关系是多对一

### 虚拟文件系统

目录项、索引节点、逻辑块以及超级块，构成了 Linux 文件系统的四大基本要素。不过，为了支持各种不同的文件系统，Linux 内核在用户进程和文件系统的中间，又引入了一个抽象层，也就是虚拟文件系统 VFS（Virtual File System）。

VFS 定义了一组所有文件系统都支持的数据结构和标准接口。这样，用户进程和内核中的其他子系统，只需要跟 VFS 提供的统一接口进行交互就可以了，而不需要再关心底层各种文件系统的实现细节。

VFS 内部又通过目录项、索引节点、逻辑块以及超级块等数据结构，来管理文件。

- 目录项，记录了文件的名字，以及文件与其他目录项之间的目录关系。
- 索引节点，记录了文件的元数据。
- 逻辑块，是由连续磁盘扇区构成的最小读写单元，用来存储文件数据。
- 超级块，用来记录文件系统整体的状态，如索引节点和逻辑块的使用情况等。

> 其中，目录项是一个内存缓存；而超级块、索引节点和逻辑块，都是存储在磁盘中的持久化数据。

### 文件系统I/O

第一，根据是否利用标准库缓存

- 缓冲 I/O，是指利用标准库缓存来加速文件的访问，而标准库内部再通过系统调度访问文件。
- 非缓冲 I/O，是指直接通过系统调用来访问文件，不再经过标准库缓存。

第二，根据是否利用操作系统的页缓存

- 直接 I/O，是指跳过操作系统的页缓存，直接跟文件系统交互来访问文件。

- 非直接 I/O 正好相反，文件读写时，先要经过系统的页缓存，然后再由内核或额外的系统调用，真正写入磁盘。

  > 想要实现直接 I/O，需要你在系统调用中，指定 O_DIRECT 标志。如果没有设置过，默认的是非直接 I/O。

第三，根据应用程序是否阻塞自身运行

- 阻塞 I/O，是指应用程序执行 I/O 操作后，如果没有获得响应，就会阻塞当前线程，自然就不能执行其他任务。

- 所谓非阻塞 I/O，是指应用程序执行 I/O 操作后，不会阻塞当前的线程，可以继续执行其他的任务，随后再通过轮询或者事件通知的形式，获取调用的结果。

  > 访问管道或者网络套接字时，设置 O_NONBLOCK 标志，就表示用非阻塞方式访问；而如果不做任何设置，默认的就是阻塞访问。

第四，根据是否等待响应结果

- 同步 I/O，是指应用程序执行 I/O 操作后，要一直等到整个 I/O 完成后，才能获得I/O 响应。

- 异步 I/O，是指应用程序执行 I/O 操作后，不用等待完成和完成后的响应，而是继续执行就可以。等到这次 I/O 完成后，响应会用事件通知的方式，告诉应用程序。

  > 在操作文件时，如果你设置了 O_SYNC 或者 O_DSYNC 标志，就代表同步I/O。如果设置了 O_DSYNC，就要等文件数据写入磁盘后，才能返回；而 O_SYNC，则是在 O_DSYNC 基础上，要求文件元数据也要写入磁盘后，才能返回。在访问管道或者网络套接字时，设置了 O_ASYNC 选项后，相应的 I/O 就是异步 I/O。

### 磁盘

磁盘是可以持久化存储的设备，根据存储介质的不同，常见磁盘可以分为两类:

- 第一类，机械磁盘，也称为硬盘驱动器（Hard Disk Driver），通常缩写为 HDD。机械磁盘主要由盘片和读写磁头组成，数据就存储在盘片的环状磁道中。在读写数据前，需要移动读写磁头，定位到数据所在的磁道，然后才能访问数据。
- 第二类，固态磁盘（Solid State Disk），通常缩写为 SSD，由固态电子元器件组成。固态磁盘不需要磁道寻址，所以，不管是连续 I/O，还是随机 I/O 的性能，都比机械磁盘要好得多。

### 通用块层

为了减小不同块设备的差异带来的影响，Linux 通过一个统一的通用块层，来管理各种不同的块设备。

通用块层，其实是处在文件系统和磁盘驱动中间的一个块设备抽象层。它主要有两个功能。

- 第一个功能跟虚拟文件系统的功能类似。向上，为文件系统和应用程序，提供访问块设备的标准接口；向下，把各种异构的磁盘设备抽象为统一的块设备，并提供统一框架来管理这些设备的驱动程序。
- 第二个功能，通用块层还会给文件系统和应用程序发来的 I/O 请求排队，并通过重新排序、请求合并等方式，提高磁盘读写的效率。

### I/O 调度算法

- 第一种 NONE ，更确切来说，并不能算 I/O 调度算法。因为它完全不使用任何 I/O 调度器，对文件系统和应用程序的 I/O 其实不做任何处理，常用在虚拟机中（此时磁盘 I/O 调度完全由物理机负责）。
- 第二种 NOOP ，是最简单的一种 I/O 调度算法。它实际上是一个先入先出的队列，只做一些最基本的请求合并，常用于 SSD 磁盘。
- 第三种 CFQ（Completely Fair Scheduler），也被称为完全公平调度器，是现在很多发行版的默认 I/O 调度器，它为每个进程维护了一个 I/O 调度队列，并按照时间片来均匀分布每个进程的 I/O 请求。
- 第四种 DeadLine 调度算法，分别为读、写请求创建了不同的 I/O 队列，可以提高机械磁盘的吞吐量，并确保达到最终期限（deadline）的请求被优先处理。DeadLine 调度算法，多用在 I/O 压力比较重的场景，比如数据库等。

### I/O 栈

可以把 Linux 存储系统的 I/O 栈，由上到下分为三个层次

- 文件系统层，包括虚拟文件系统和其他各种文件系统的具体实现。它为上层的应用程序，提供标准的文件访问接口；对下会通过通用块层，来存储和管理磁盘数据。
- 通用块层，包括块设备 I/O 队列和 I/O 调度器。它会对文件系统的 I/O 请求进行排队，再通过重新排序和请求合并，然后才要发送给下一级的设备层。
- 设备层，包括存储设备和相应的驱动程序，负责最终物理设备的 I/O 操作。

## 文件系统 I/O 性能指标

### 磁盘空间的使用量

存储空间的使用情况，包括容量、使用量以及剩余空间等

### 索引节点的使用情况

包括容量、使用量以及剩余量等三个指标

### 缓存使用情况

包括页缓存、目录项缓存、索引节点缓存以及各个具体文件系统（如 ext4、XFS 等）的缓存

### 文件 I/O

包括 IOPS（包括 r/s 和 w/s）、响应时间（延迟）以及吞吐量（B/s）等

## 磁盘 I/O 性能指标

### 使用率

是指磁盘处理 I/O 的时间百分比。过高的使用率（比如超过 80%），通常意味着磁盘 I/O 存在性能瓶颈。使用率只考虑有没有 I/O，而不考虑 I/O 的大小。换句话说当使用率是 100% 的时候，磁盘依然有可能接受新的 I/O 请求。

### 饱和度

是指磁盘处理 I/O 的繁忙程度。过高的饱和度，意味着磁盘存在严重的性能瓶颈。当饱和度为 100% 时，磁盘无法接受新的 I/O 请求。

### IOPS（Input/Output Per Second）

是指每秒的 I/O 请求数。

### 吞吐量

是指每秒的 I/O 请求大小。

### 响应时间

是指 I/O 请求从发出到收到响应的间隔时间。

## 性能工具

从故障案例的角度来看使用的性能工具。

第一，在文件系统的原理中，我介绍了查看文件系统容量的工具 df。它既可以查看文件系统数据的空间容量，也可以查看索引节点的容量。至于文件系统缓存，我们通过/proc/meminfo、/proc/slabinfo 以及 slabtop 等各种来源，观察页缓存、目录项缓存、索引节点缓存以及具体文件系统的缓存情况。

第二，在磁盘 I/O 的原理中，我们分别用 iostat 和 pidstat 观察了磁盘和进程的 I/O 情况。它们都是最常用的 I/O 性能分析工具。通过 iostat ，我们可以得到磁盘的 I/O 使用率、吞吐量、响应时间以及 IOPS 等性能指标；而通过 pidstat ，则可以观察到进程的 I/O吞吐量以及块设备 I/O 的延迟等。

第三，在狂打日志的案例中，我们先用 top 查看系统的 CPU 使用情况，发现 iowait 比较高；然后，又用 iostat 发现了磁盘的 I/O 使用率瓶颈，并用 pidstat 找出了大量 I/O 的进程；最后，通过 strace 和 lsof，我们找出了问题进程正在读写的文件，并最终锁定性能问题的来源——原来是进程在狂打日志。

第四，在磁盘 I/O 延迟的单词热度案例中，我们同样先用 top、iostat ，发现磁盘有 I/O瓶颈，并用 pidstat 找出了大量 I/O 的进程。可接下来，想要照搬上次操作的我们失败了。在随后的 strace 命令中，我们居然没看到 write 系统调用。于是，我们换了一个思路，用新工具 filetop 和 opensnoop ，从内核中跟踪系统调用，最终找出瓶颈的来源。

从文件系统和磁盘 I/O 的性能指标出发，也就是说当你想查看某项指标的时候，该用哪些工具

| 性能指标                                                     | 工具                      | 说明                                     |
| :----------------------------------------------------------- | :------------------------ | :--------------------------------------- |
| 文件系统空间容量、使用量以及剩余空间                         | df                        | 详细文档见info coreutils ‘df invocation’ |
| 索引节点容量、使用量以及剩余量                               | df                        | 使用-i选项                               |
| 页缓存和可回收Slab缓存                                       | /proc/meminfo sar、vmstat | 使用sar -r选项                           |
| 缓冲区                                                       | /proc/meminfo sar、vmstat | 使用sar -r选项                           |
| 目录项、索引节点以及文件系统的缓存                           | /proc/slabinfo slabtop    | slabtop更直观                            |
| 磁盘I/0使用率、IOPS、吞吐量、响应时间、I/O平均大小以及等待队列长度 | iostat sar、dstat         | 使用iostat -d-x或sar-d选项               |
| 进程/0大小以及I/O延迟                                        | pidstat iotop             | 使用pidstat -d选项                       |
| 块设备/0事件跟踪                                             | blktrace                  | 示例: blktrace -d /dev/sda               |
| 进程I/O系统调用跟踪                                          | strace                    | 通过系统调用跟踪进程的I/O                |
| 进程块设备I/O大小跟踪                                        | biosnoop biotop           | 需要安装bcc软件包                        |

从工具出发。也就是当你已经安装了某个工具后，要知道这个工具能提供哪些指标。

| 性能工具        | 性能指标                                                     |
| :-------------- | :----------------------------------------------------------- |
| iostat          | 磁盘I/O使用率、IOPS、 吞吐量、响应时间、I/O平均大小以及等待队列长度 |
| pidstat         | 进程I/O大小以及I/O延迟                                       |
| sar             | 磁盘I/O使用率、IOPS 、吞吐量以及响应时间                     |
| dstat           | 磁盘I/O使用率、IOPS以及吞吐量                                |
| iotop           | 按I/O大小对进程排序                                          |
| slabtop         | 目录项、索引节点以及文件系统的缓存                           |
| /proc/slabinfo  | 目录项、索引节点以及文件系统的缓存                           |
| /proc/meminfo   | 页缓存和可回收Slab缓存                                       |
| /proc/diskstats | 磁盘的IOPS、吞吐量以及延迟!                                  |
| /proc/pid/io    | 进程IOPS、IO大小以及IO延迟                                   |
| vmstat          | 缓存和缓冲区用量汇总                                         |
| blktrace        | 跟踪块设备I/O事件                                            |
| biosnoop        | 跟踪进程的块设备I/O大小                                      |
| biotop          | 跟踪进程块I/O并按I/O大小排序                                 |
| strace          | 跟踪进程的I/O系统调用                                        |
| perf            | 跟踪内核中的I/O事件                                          |
| df              | 磁盘空间和索引节点使用量和剩余量                             |
| mount           | 文件系统的挂载路径以及挂载参数                               |
| du              | 目录占用的磁盘空间大小                                       |
| tune2fs         | 显示和设置文件系统参数                                       |
| hdparam         | 显示和设置磁盘参数                                           |

## 如何迅速分析 IO 的性能瓶颈

1. 先用 iostat 发现磁盘 I/O 性能瓶颈；
2. 再借助 pidstat ，定位出导致瓶颈的进程；
3. 随后分析进程的 I/O 行为；
4. 最后，结合应用程序的原理，分析这些 I/O 的来源。

![io-tools](https://lework.github.io/assets/images/linux/io-tools.png)

## IO 性能优化

### io的基准数据

在优化之前，需要得到文件系统或者磁盘 I/O 的极限性能。使用fio可以获取到这些数据。

fio（Flexible I/O Tester）正是最常用的文件系统和磁盘 I/O 性能基准测试工具。它提供了大量的可定制化选项，可以用来测试，裸盘或者文件系统在各种场景下的 I/O 性能，包括了不同块大小、不同 I/O 引擎以及是否使用缓存等场景。

常用命令

```
# 随机读
fio -name=randread -direct=1 -iodepth=64 -rw=randread -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sda

# 随机写
fio -name=randwrite -direct=1 -iodepth=64 -rw=randwrite -ioengine=libaio -bs=4k -size=1G 
-numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sda

# 顺序读
fio -name=read -direct=1 -iodepth=64 -rw=read -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sda

# 顺序写
fio -name=write -direct=1 -iodepth=64 -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sda
COPY
```

参数解释

- direct，表示是否跳过系统缓存。上面示例中，我设置的 1 ，就表示跳过系统缓存。
- iodepth，表示使用异步 I/O（asynchronous I/O，简称 AIO）时，同时发出的 I/O 请求上限。在上面的示例中，我设置的是 64。
- rw，表示 I/O 模式。我的示例中， read/write 分别表示顺序读 / 写，而randread/randwrite 则分别表示随机读 / 写。
- ioengine，表示 I/O 引擎，它支持同步（sync）、异步（libaio）、内存映射（mmap）、网络（net）等各种 I/O 引擎。上面示例中，我设置的 libaio 表示使用异步 I/O。
- bs，表示 I/O 的大小。示例中，我设置成了 4K（这也是默认值）。
- filename，表示文件路径

### 应用程序优化

- 第一，可以用追加写代替随机写，减少寻址开销，加快 I/O 写的速度。
- 第二，可以借助缓存 I/O ，充分利用系统缓存，降低实际 I/O 的次数。
- 第三，可以在应用程序内部构建自己的缓存，或者用 Redis 这类外部缓存系统。这样，一方面，能在应用程序内部，控制缓存的数据和生命周期；另一方面，也能降低其他应用程序使用缓存对自身的影响。
- 第四，在需要频繁读写同一块磁盘空间时，可以用 mmap 代替 read/write，减少内存的拷贝次数。
- 第五，在需要同步写的场景中，尽量将写请求合并，而不是让每个请求都同步写入磁盘，即可以用 fsync() 取代 O_SYNC。
- 第六，在多个应用程序共享相同磁盘时，为了保证 I/O 不被某个应用完全占用，推荐你使用 cgroups 的 I/O 子系统，来限制进程 / 进程组的 IOPS 以及吞吐量。
- 在使用 CFQ 调度器时，可以用 ionice 来调整进程的 I/O 调度优先级，特别是提高核心应用的 I/O 优先级。ionice 支持三个优先级类：Idle、Best-effort 和 Realtime。其中， Best-effort 和 Realtime 还分别支持 0-7 的级别，数值越小，则表示优先级别越高。

### 文件系统优化

- 第一，你可以根据实际负载场景的不同，选择最适合的文件系统。比如 Ubuntu 默认使用ext4 文件系统，而 CentOS 7 默认使用 xfs 文件系统。
- 第二，在选好文件系统后，还可以进一步优化文件系统的配置选项，包括文件系统的特性（如 ext_attr、dir_index）、日志模式（如 journal、ordered、writeback）、挂载选项（如 noatime）等等。
- 第三，可以优化文件系统的缓存。
- 在不需要持久化时，你还可以用内存文件系统 tmpfs，以获得更好的 I/O 性能 。

### 磁盘优化

- 第一，最简单有效的优化方法，就是换用性能更好的磁盘，比如用 SSD 替代 HDD。
- 第二，我们可以使用 RAID ，把多块磁盘组合成一个逻辑磁盘，构成冗余独立磁盘阵列。这样做既可以提高数据的可靠性，又可以提升数据的访问性能。
- 第三，针对磁盘和应用程序 I/O 模式的特征，我们可以选择最适合的 I/O 调度算法。比方说，SSD 和虚拟机中的磁盘，通常用的是 noop 调度算法。而数据库应用，我更推荐使用deadline 算法。
- 第四，我们可以对应用程序的数据，进行磁盘级别的隔离。比如，我们可以为日志、数据库等 I/O 压力比较重的应用，配置单独的磁盘。
- 第五，在顺序读比较多的场景中，我们可以增大磁盘的预读数据
- 第六，我们可以优化内核块设备 I/O 的选项。比如，可以调整磁盘队列的长度/sys/block/sdb/queue/nr_requests，适当增大队列长度，可以提升磁盘的吞吐量（当然也会导致 I/O 延迟增大）。

## 性能工具的使用

### iostat

> iostat是I/O statistics（输入/输出统计）的缩写，iostat工具将对系统的磁盘操作活动进行监视。它的特点是汇报磁盘活动统计情况，同时也会汇报出CPU使用情况

查看磁盘i/o `iostat -d -x 1`

| 性能指标 | 含义                                      | 提示                                                       |
| :------- | :---------------------------------------- | :--------------------------------------------------------- |
| r/s      | 每秒发送给磁盘的读请求数                  | 合并后的请求数                                             |
| w/s      | 每秒发送给磁盘的写请求数                  | 合并后的请求数                                             |
| rkB/s    | 每秒从磁盘读取的数据量                    | 单位为kB                                                   |
| wkB/s    | 每秒向磁盘写入的数据量                    | 单位为kB                                                   |
| rrqm/s   | 每秒合并的读请求数                        | %rrqm表示合并读请求的百分比                                |
| wrqm/s   | 每秒合并的写请求数                        | %wrqm表示合并写请求的百分比                                |
| r_await  | 读请求处理完成等待时间                    | 包括队列中的等待时间和设备实际处理的时间，单位为毫秒       |
| w_await  | 写请求处理完成等待时间                    | 包括队列中的等待时间和设备实际处理的时间，单位为毫秒       |
| aqu-sz   | 平均请求队列长度                          | 旧版中为avgqu-sz                                           |
| rareq-sz | 平均读请求大小                            | 单位为kB                                                   |
| wareq-sz | 平均写请求大小                            | 单位为kB                                                   |
| svctm    | 处理I/O请求所需的平均时间(不包括等待时间) | 单位为毫秒。注意这是推断的数据，并不保证完全准确           |
| %util    | 磁盘处理I/O的时间百分比                   | 即使用率，由于可能存在并行I/O，100%并不一定表明磁盘I/O饱和 |

- %util ，就是我们前面提到的磁盘 I/O 使用率；
- r/s+ w/s ，就是 IOPS；
- rkB/s+wkB/s ，就是吞吐量；
- r_await+w_await ，就是响应时间。

### pidstat

> pidstat是sysstat工具的一个命令，用于监控全部或指定进程的cpu、内存、线程、设备IO等系统资源的占用情况。pidstat首次运行时显示自系统启动开始的各项统计信息，之后运行pidstat将显示自上次运行该命令以后的统计信息。用户可以通过指定统计的次数和时间来获得所需的统计信息。

显示各个进程的io使用情况

```
pidstat -d
```

结果

- PID：进程id
- kB_rd/s：每秒从磁盘读取的KB
- kB_wr/s：每秒写入磁盘KB
- kB_ccwr/s：任务取消的写入磁盘的KB。当任务截断脏的pagecache的时候会发生。
- COMMAND: task的命令名

### sar

> sar是System Activity Reporter（系统活动情况报告）的缩写。sar工具将对系统当前的状态进行取样，然后通过计算数据和比例来表达系统的当前运行状态。

查看IO和传递速率 `sar -b 1 3`

- tps 磁盘每秒钟的IO总数，等于iostat中的tps
- rtps 每秒钟从磁盘读取的IO总数
- wtps 每秒钟从写入到磁盘的IO总数
- bread/s 每秒钟从磁盘读取的块总数
- bwrtn/s 每秒钟此写入到磁盘的块总数

查看磁盘使用情况 `sar -d 1 3`

- DEV 磁盘设备的名称，如果不加-p，会显示dev253-0类似的设备名称，因此加上-p显示的名称更直接
- tps 每秒I/O的传输总数
- rd_sec/s 每秒读取的扇区的总数
- wr_sec/s 每秒写入的扇区的总数
- avgrq-sz 平均每次次磁盘I/O操作的数据大小（扇区）
- avgqu-sz 磁盘请求队列的平均长度
- await 从请求磁盘操作到系统完成处理，每次请求的平均消耗时间，包括请求队列等待时间，单位是毫秒（1秒等于1000毫秒），等于寻道时间+队列时间+服务时间
- svctm I/O的服务处理时间，即不包括请求队列中的时间
- %util I/O请求占用的CPU百分比，值越高，说明I/O越慢

### iotop

> iotop是一个用来监视磁盘I/O使用状况的 top 类工具，可监测到哪一个程序使用的磁盘IO的信息

```
iotop -o -d 2 -n 5
各个参数说明：

　　-o, --only只显示正在产生I/O的进程或线程。除了传参，可以在运行过程中按o生效。
　　-b, --batch非交互模式，一般用来记录日志。
　　-n NUM, --iter=NUM设置监测的次数，默认无限。在非交互模式下很有用。
　　-d SEC, --delay=SEC设置每次监测的间隔，默认1秒，接受非整形数据例如1.1。
　　-p PID, --pid=PID指定监测的进程/线程。
　　-u USER, --user=USER指定监测某个用户产生的I/O。
　　-P, --processes仅显示进程，默认iotop显示所有线程。
　　-a, --accumulated显示累积的I/O，而不是带宽。
　　-k, --kilobytes使用kB单位，而不是对人友好的单位。在非交互模式下，脚本编程有用。
　　-t, --time 加上时间戳，非交互非模式。
　　-q, --quiet 禁止头几行，非交互模式。有三种指定方式。
　　-q 只在第一次监测时显示列名
　　-qq 永远不显示列名。
　　-qqq 永远不显示I/O汇总。
交互按键：
　　和top命令类似，iotop也支持以下几个交互按键。
　　left和right方向键：改变排序。　　
　　r：反向排序。
　　o：切换至选项--only。
　　p：切换至--processes选项。
　　a：切换至--accumulated选项。
　　q：退出。
　　i：改变线程的优先级。
COPY
```

### **slabtop**

> slabtop实时显示详细的内核板条缓存信息。它显示按所列排序条件之一排序的顶级缓存的列表。它还会显示一个统计信息头，其中填充了板坯层信息。

```
选项：
--delay=n, -d n	#每n秒更新一次显示的信息，默认是每3秒
--sort=S, -s S	#指定排序标准进行排序（排序标准，参照下面或者man手册）
--once, -o	#显示一次后退出
--version, -V	#显示版本
--help	#显示帮助信息

排序标准：
a: sort by number of active objects
b: sort by objects per slab
c: sort by cache size
l: sort by number of slabs
v： sort by number of active slabs
n: sort by name
o: sort by number of objects
p: sort by pages per slab
s: sort by object size
u: sort by cache utilization

输出界面可用的命令：
<SPACEBAR>：	刷新显示内容
Q：	退出
COPY
```

### df

> 用于显示目前在Linux系统上的文件系统的磁盘使用情况统计。

```
命令参数
-a, --all 包含所有的具有 0 Blocks 的文件系统
--block-size={SIZE} 使用 {SIZE} 大小的 Blocks
-h, --human-readable 使用人类可读的格式(预设值是不加这个选项的...)
-H, --si 很像 -h, 但是用 1000 为单位而不是用 1024
-i, --inodes 列出 inode 资讯，不列出已使用 block
-k, --kilobytes 就像是 --block-size=1024
-l, --local 限制列出的文件结构
-m, --megabytes 就像 --block-size=1048576
--no-sync 取得资讯前不 sync (预设值)
-P, --portability 使用 POSIX 输出格式
--sync 在取得资讯前 sync
-t, --type=TYPE 限制列出文件系统的 TYPE
-T, --print-type 显示文件系统的形式
-x, --exclude-type=TYPE 限制列出文件系统不要显示 TYPE
-v (忽略)
--help 显示这个帮手并且离开
--version 输出版本资讯并且离开
COPY
```

### du

> 查看文件的磁盘使用情况。

```
选项：
-a：对所有文件进行统计，而不仅仅是目录；
-B size：使用size字节为一个块，进行统计；
-b：已字节（byte）为计数单位；
-k：已KB（1024byte）为计数单位；
-m：已MB为计数单位；
-c：显示所有文件和目录的大小总和；（就是在最后显示个“总大小　　total”好像然并卵）
-D：显示符号链接指向的源文件大小；
-h：已人类可读的方式进行显示；（K，M，G）
-s：仅显示总大小；
-l：重复计算硬链接文件大小；
-L：显示符号链接所指向文件的大小；
-0：不换行输出；
-S：显示目录大小时，不包含子目录大小；
--max-depth N:显示的最大层数；
--time [WORD]：默认显示修改时间，WORD可以设置成ctime、atime；
--time-style STYLE：显示时间使用样式STYLE：full-iso，long-iso，iso，+ FORMAT FORMAT被解释为`date`
COPY
```

**原文地址** https://lework.github.io/2020/01/11/io/